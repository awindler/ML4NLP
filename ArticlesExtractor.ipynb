{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class Element:\n",
    "    def __init__(self, name, url):\n",
    "        self.name = name.strip()\n",
    "        self.url = url\n",
    "    \n",
    "    def get_url(self):\n",
    "        return Element.convert_url(self.url)\n",
    "        \n",
    "    def convert_url(relative_url):\n",
    "        return 'https://en.wikinews.org{}'.format(relative_url)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return '\"{}\"'.format(self.name)\n",
    "    \n",
    "class Article(Element):\n",
    "    def __init__(self, name, url):\n",
    "        super().__init__(name, url)\n",
    "    \n",
    "    def is_meaningfull(self):\n",
    "        if not self.url.startswith('/wiki/'):\n",
    "            return False\n",
    "\n",
    "        if self.name.startswith('Template') \\\n",
    "        or self.name.startswith('Portal') \\\n",
    "        or self.name.startswith('User') \\\n",
    "        or self.name.startswith('Talk') \\\n",
    "        or self.name.startswith('Help') \\\n",
    "        or self.name.startswith('TEST') \\\n",
    "        or self.name.startswith('Category') \\\n",
    "        or self.name.startswith('Module') \\\n",
    "        or self.name.startswith('News'):\n",
    "            return False\n",
    "\n",
    "        if 'Wikinews' in self.name:\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def __str__(self):\n",
    "        return '\"{}\"'.format(self.name)\n",
    "        \n",
    "class Category(Element):\n",
    "    def __init__(self, name, url, sub_categories=[], articles=[]):\n",
    "        super().__init__(name, url)\n",
    "        self.sub_categories = sub_categories\n",
    "        self.articles = articles\n",
    "    \n",
    "    def parse(self, articles_cache, categories_cache):\n",
    "        page_html = requests.get(self.get_url(), timeout=5)\n",
    "        page_content = BeautifulSoup(page_html.content, \"html.parser\")\n",
    "\n",
    "        # Parse and add unknown articles\n",
    "        mw_pages = page_content.find('div', {\"id\": \"mw-pages\"})\n",
    "        if mw_pages is not None:\n",
    "            article_candidates = [Article(name=article.text, url=article['href']) for article in mw_pages.findAll('a')]\n",
    "            articles = {\n",
    "                article.url: article\n",
    "                for article in article_candidates\n",
    "                if article.is_meaningfull()\n",
    "            }\n",
    "            articles_cache.update(articles)\n",
    "        \n",
    "            # Add childs\n",
    "            self.articles = list(articles.keys())\n",
    "        \n",
    "        mw_subcategories = page_content.find('div', {\"id\": \"mw-subcategories\"})\n",
    "        if mw_subcategories is not None:\n",
    "            self.sub_categories = [\n",
    "                category['href']\n",
    "                for category in mw_subcategories.findAll('a')\n",
    "                if category['href'] in categories_cache\n",
    "            ]\n",
    "            \n",
    "        return True\n",
    "    \n",
    "    def from_wikinews(limit = 2000, num_categories=-1):\n",
    "        WIKINEWS = \"https://en.wikinews.org/w/index.php?title=Special:Categories&limit={}&offset={}\"\n",
    "\n",
    "        category_detector = re.compile('\\/wiki\\/Category:.+')\n",
    "        number_detector = re.compile('\\(([0-9,]+) member')\n",
    "        categories = []\n",
    "\n",
    "        while (num_categories == -1 or len(categories) < num_categories):\n",
    "            url = WIKINEWS.format(limit, categories[-1].url[15:] if len(categories) > 0 else '')\n",
    "            page_response = requests.get(url, timeout=5)\n",
    "            page_content = BeautifulSoup(page_response.content, \"html.parser\").find('div', {\"class\": \"mw-spcontent\"})\n",
    "\n",
    "            for candidate in page_content.find_all('a', {\"href\": category_detector}):\n",
    "                num_childs = int(number_detector.search(candidate.parent.text).group(1).replace(',', ''))\n",
    "                if num_childs > 0:\n",
    "                    categories.append(Category(url=candidate['href'], name=candidate.text))\n",
    "\n",
    "            if page_content.find('a', {'class':'mw-nextlink'}) is None:\n",
    "                break\n",
    "\n",
    "        return { category.url : category for category in categories }\n",
    "\n",
    "    def __str__(self):\n",
    "        return '\"{}\":\\n\\tArticles: {}\\n\\tSubcategories: {}\\n'.format(self.name, self.articles, self.sub_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_categories():\n",
    "    categories = Category.from_wikinews()\n",
    "    print(\"Complete:\\t\\t\", len(categories))\n",
    "\n",
    "    date_regex = re.compile('^((January)|(February)|(March)|(April)|(May)|(June)|(July)|(August)|(September)|(October)|(November)|(December))')\n",
    "    year_regex = re.compile('^[0-9]{4}$')\n",
    "    filtered = {key: value for key, value in categories.items() if not date_regex.match(value.name)}\n",
    "    filtered = {key: value for key, value in filtered.items() if not year_regex.match(value.name)}\n",
    "    print(\"Without dates:\\t\\t\", len(filtered))\n",
    "\n",
    "    wiki_regex = re.compile('.*[w|W]iki.*')\n",
    "    filtered = {key: value for key, value in filtered.items() if not wiki_regex.match(value.name)}\n",
    "    print(\"Without 'wiki':\\t\\t\", len(filtered))\n",
    "\n",
    "    wiki_other_noise = re.compile('.*((Dialog )|(Files)|(Sockpuppets)|(Sources\\/)|(Requests for)|(Peer reviewed)|(Non-)|(News articles)|(Media)|(Categor[y|(ies)])|(CC[-| ]BY)|(Assistant)|(Pages*)|(Checkuser)|(Template)|(WWC)|(Failed)|(Abandoned)|(Local)|(Live)|(Wikinewsie)|(User)|(UoW)).*', re.IGNORECASE)\n",
    "    filtered = {key: value for key, value in filtered.items() if not wiki_other_noise.match(value.name)}\n",
    "    print(\"Without noise:\\t\\t\", len(filtered))\n",
    "\n",
    "    #TO BE REMOVED\n",
    "    # Move To Commons\n",
    "    # Module documentation\n",
    "    # No publish\n",
    "    # Disputed\n",
    "    # Archive[s|d]\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "FILE_CACHE = Path(\"wikinews.pickle\")\n",
    "\n",
    "if not FILE_CACHE.exists():\n",
    "    print(\"Loading from Wikinews...\")\n",
    "    categories = load_categories()\n",
    "    articles = {}\n",
    "    \n",
    "    for category in categories.values():\n",
    "        category.parse(articles, categories)\n",
    "    \n",
    "    with open(FILE_CACHE, \"wb\") as file:\n",
    "        pickle.dump((categories, articles), file)\n",
    "else:\n",
    "    print(\"Loading from cache...\")\n",
    "    with open(FILE_CACHE, \"rb\") as file:\n",
    "        categories, articles = pickle.load(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
