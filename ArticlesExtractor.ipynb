{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class Element:\n",
    "    def __init__(self, name, url):\n",
    "        self.name = name\n",
    "        self.url = url\n",
    "    \n",
    "    def get_url(self):\n",
    "        return Element.convert_url(self.url)\n",
    "        \n",
    "    def convert_url(relative_url):\n",
    "        return 'https://en.wikinews.org{}'.format(self.url)\n",
    "\n",
    "class Article(Element):\n",
    "    def __init__(self, name, url):\n",
    "        super().__init__(name, url)\n",
    "        \n",
    "class Category(Element):\n",
    "    def __init__(self, name, url, sub_categories=[], articles=[]):\n",
    "        super().__init__(name, url)\n",
    "        self.sub_categories = sub_categories\n",
    "        self.articles = articles\n",
    "    \n",
    "    def parse(self, articles_cache, categories_cache):\n",
    "        page_html = requests.get(self.get_url(), timeout=5)\n",
    "        page_content = BeautifulSoup(page_html.content, \"html.parser\")\n",
    "\n",
    "        # Read and add articles\n",
    "        articles = {\n",
    "            article['href']: Article(name=article.text, url=article['href'])\n",
    "            for article in page_content.find('div', {\"id\": \"mw-pages\"}).findAll('a')\n",
    "        }\n",
    "        articles_cache.update(articles)\n",
    "        self.sub_categories = list(categories.keys())\n",
    "        \n",
    "        # Read all categories\n",
    "        self.articles = [\n",
    "            category['href']\n",
    "            for category in page_content.find('div', {\"id\": \"mw-subcategories\"}).findAll('a')\n",
    "            if category['href'] in categories_cache\n",
    "        ]\n",
    "    \n",
    "    def from_wikinews(limit = 2000, num_categories=-1):\n",
    "        WIKINEWS = \"https://en.wikinews.org/w/index.php?title=Special:Categories&limit={}&offset={}\"\n",
    "\n",
    "        category_detector = re.compile('\\/wiki\\/Category:.+')\n",
    "        number_detector = re.compile('\\(([0-9,]+) member')\n",
    "        categories = []\n",
    "\n",
    "        while (num_categories == -1 or len(categories) < num_categories):\n",
    "            url = WIKINEWS.format(limit, categories[-1].url[15:] if len(categories) > 0 else '')\n",
    "            page_response = requests.get(url, timeout=5)\n",
    "            page_content = BeautifulSoup(page_response.content, \"html.parser\").find('div', {\"class\": \"mw-spcontent\"})\n",
    "\n",
    "            for candidate in page_content.find_all('a', {\"href\": category_detector}):\n",
    "                num_childs = int(number_detector.search(candidate.parent.text).group(1).replace(',', ''))\n",
    "                if num_childs > 0:\n",
    "                    categories.append(Category(url=candidate['href'], name=candidate.text))\n",
    "\n",
    "            if page_content.find('a', {'class':'mw-nextlink'}) is None:\n",
    "                break\n",
    "\n",
    "        return { category.url : category for category in categories }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = Category.from_wikinews()\n",
    "print(\"Complete:\\t\\t\", len(categories))\n",
    "\n",
    "date_regex = re.compile('^((January)|(February)|(March)|(April)|(May)|(June)|(July)|(August)|(September)|(October)|(November)|(December))')\n",
    "year_regex = re.compile('^[0-9]{4}$')\n",
    "filtered = {key: value for key, value in categories.items() if not date_regex.match(value.name)}\n",
    "filtered = {key: value for key, value in filtered.items() if not year_regex.match(value.name)}\n",
    "print(\"Without dates:\\t\\t\", len(filtered))\n",
    "\n",
    "wiki_regex = re.compile('.*[w|W]iki.*')\n",
    "filtered = {key: value for key, value in filtered.items() if not wiki_regex.match(value.name)}\n",
    "print(\"Without 'wiki':\\t\\t\", len(filtered))\n",
    "\n",
    "wiki_other_noise = re.compile('.*((Dialog )|(Files)|(Sockpuppets)|(Sources\\/)|(Requests for)|(Peer reviewed)|(Non-)|(News articles)|(Media)|(Categor[y|(ies)])|(CC[-| ]BY)|(Assistant)|(Pages*)|(Checkuser)|(Template)|(WWC)|(Failed)|(Abandoned)|(Local)|(Live)|(Wikinewsie)|(User)|(UoW)).*', re.IGNORECASE)\n",
    "filtered = {key: value for key, value in filtered.items() if not wiki_other_noise.match(value.name)}\n",
    "print(\"Without noise:\\t\\t\", len(filtered))\n",
    "\n",
    "#TO BE REMOVED\n",
    "# Move To Commons\n",
    "# Module documentation\n",
    "# No publish\n",
    "# Disputed\n",
    "# Archive[s|d]\n",
    "\n",
    "print()\n",
    "for category in filtered.values():\n",
    "    print(category.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
