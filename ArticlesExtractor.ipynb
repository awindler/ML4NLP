{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from multiprocessing import Pool\n",
    "import json\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class Element:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "    \n",
    "    def get_url(self):\n",
    "        return Element.convert_url(self.url)\n",
    "        \n",
    "    def convert_url(relative_url):\n",
    "        return 'https://en.wikinews.org{}'.format(relative_url)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    \n",
    "    def __str__(self):\n",
    "        return '\"{}\"'.format(self.url)\n",
    "    \n",
    "class Article(Element):\n",
    "    def __init__(self, url, name=None, categories=[]):\n",
    "        super().__init__(url)\n",
    "        \n",
    "        if name is not None:\n",
    "            self.name = name.strip()\n",
    "            self.categories = categories\n",
    "        else:\n",
    "            self.name, self.categories = Article.parse(url)\n",
    "    \n",
    "    def is_meaningfull(self):\n",
    "        if not self.url.startswith('/wiki/'):\n",
    "            return False\n",
    "\n",
    "        if self.name.startswith('Template') \\\n",
    "        or self.name.startswith('Portal') \\\n",
    "        or self.name.startswith('User') \\\n",
    "        or self.name.startswith('Talk') \\\n",
    "        or self.name.startswith('Help') \\\n",
    "        or self.name.startswith('TEST') \\\n",
    "        or self.name.startswith('Category') \\\n",
    "        or self.name.startswith('Module') \\\n",
    "        or self.name.startswith('News'):\n",
    "            return False\n",
    "\n",
    "        if 'Wikinews' in self.name:\n",
    "            return False\n",
    "\n",
    "        return True\n",
    "\n",
    "    def parse(url):    \n",
    "        json_url = \"https://en.wikinews.org/w/api.php?action=query&titles={}&prop=categories&format=json\".format(\n",
    "            url[6:]\n",
    "        )\n",
    "        \n",
    "        response = json.loads(requests.get(json_url, timeout=5).content)\n",
    "        pages = response['query']['pages']\n",
    "        if len(pages) != 1:\n",
    "            raise \"\"\n",
    "        page = next(iter(pages.values()))\n",
    "        \n",
    "        categories = page['categories'] if 'categories' in page else []\n",
    "        return page['title'], [category['title'][9:] for category in categories]\n",
    "            \n",
    "    def __str__(self):\n",
    "        return '\"{}\"'.format(self.name)\n",
    "        \n",
    "class Category(Element):\n",
    "    def __init__(self, url, name=None, sub_categories=[], articles=[]):\n",
    "        super().__init__(url)\n",
    "        self.name = name\n",
    "        self.sub_categories = sub_categories\n",
    "        self.articles = articles\n",
    "    \n",
    "    def parse(self, articles_cache, categories_cache, check_article=True, check_categories=True):\n",
    "        page_html = requests.get(self.get_url(), timeout=5)\n",
    "        page_content = BeautifulSoup(page_html.content, \"html.parser\")\n",
    "\n",
    "        if self.name is None:\n",
    "            title_element = page_content.find('h1')\n",
    "            if title_element is not None:\n",
    "                self.name = title_element.text[9:].strip()\n",
    "                \n",
    "        # Parse and add unknown articles\n",
    "        mw_pages = page_content.find('div', {\"id\": \"mw-pages\"})\n",
    "        if mw_pages is not None:\n",
    "            article_candidates = [Article(name=article.text, url=article['href']) for article in mw_pages.findAll('a')]\n",
    "            articles = {\n",
    "                article.url: article\n",
    "                for article in article_candidates\n",
    "                if (not check_article) or article.is_meaningfull()\n",
    "            }\n",
    "            articles_cache.update(articles)\n",
    "        \n",
    "            # Add childs\n",
    "            self.articles = list(articles.keys())\n",
    "        \n",
    "        mw_subcategories = page_content.find('div', {\"id\": \"mw-subcategories\"})\n",
    "        if mw_subcategories is not None:\n",
    "            self.sub_categories = [\n",
    "                category['href']\n",
    "                for category in mw_subcategories.findAll('a')\n",
    "                if (not check_categories) or category['href'] in categories_cache\n",
    "            ]\n",
    "            \n",
    "            if not check_categories:\n",
    "                new_categories = [\n",
    "                    Category(category_url) for category_url in self.sub_categories \n",
    "                    if category_url not in categories_cache\n",
    "                ]\n",
    "                \n",
    "                categories_cache.update({\n",
    "                    category.url: category \n",
    "                    for category in new_categories \n",
    "                    if category.parse(articles_cache, categories_cache, check_article, check_categories)\n",
    "                })\n",
    "            \n",
    "        return True\n",
    "    \n",
    "    def from_urls(categories):\n",
    "        articles_tmp = {}\n",
    "        categories_tmp = {}\n",
    "        return [\n",
    "            category \n",
    "            for category in (Category(category) for category in categories)\n",
    "            if category.parse(articles_tmp, categories_tmp, False, False)\n",
    "        ], categories_tmp, articles_tmp\n",
    "            \n",
    "    def from_wikinews(limit = 2000, num_categories=-1):\n",
    "        WIKINEWS = \"http://en.wikinews.org/w/index.php?title=Special:Categories&limit={}&offset={}\"\n",
    "\n",
    "        category_detector = re.compile('\\/wiki\\/Category:.+')\n",
    "        number_detector = re.compile('\\(([0-9,]+) ')\n",
    "        categories = []\n",
    "\n",
    "        while (num_categories == -1 or len(categories) < num_categories):\n",
    "            url = WIKINEWS.format(limit, categories[-1].url[15:] if len(categories) > 0 else '')\n",
    "            page_response = requests.get(url, timeout=5)\n",
    "            page_content = BeautifulSoup(page_response.content, \"html.parser\").find('div', {\"class\": \"mw-spcontent\"})\n",
    "\n",
    "            for candidate in page_content.find_all('a', {\"href\": category_detector}):\n",
    "                num_childs = int(number_detector.search(candidate.parent.text).group(1).replace(',', ''))\n",
    "                if num_childs > 0:\n",
    "                    categories.append(Category(url=candidate['href']))\n",
    "\n",
    "            if page_content.find('a', {'class':'mw-nextlink'}) is None:\n",
    "                break\n",
    "\n",
    "        return { category.url : category for category in categories }\n",
    "\n",
    "    def filter_categories(categories, min_articles):\n",
    "        date_regex = re.compile('^((January)|(February)|(March)|(April)|(May)|(June)|(July)|(August)|(September)|(October)|(November)|(December))')\n",
    "        year_regex = re.compile('^[0-9]{4}$')\n",
    "        wiki_regex = re.compile('.*[w|W]iki.*')\n",
    "        wiki_other_noise = re.compile(\"\"\".*(\n",
    "        (Archived)|\n",
    "        (Dialog )|\n",
    "        (Files)|\n",
    "        (Sockpuppets)|\n",
    "        (Sources\\/)|\n",
    "        (Requests for)|\n",
    "        (Peer reviewed)|\n",
    "        (Non-)|\n",
    "        (articles?)|\n",
    "        (Media)|\n",
    "        (Categor[y|(ies)])|\n",
    "        (CC[-| ]BY)|\n",
    "        (Assistant)|\n",
    "        (Pages*)|\n",
    "        (Checkuser)|\n",
    "        (Template)|\n",
    "        (WWC)|\n",
    "        (Failed)|\n",
    "        (Abandoned)|\n",
    "        (Local)|\n",
    "        (Live)|\n",
    "        (Wikinewsie)|\n",
    "        (User)|\n",
    "        (UoW)|\n",
    "        (No publish)|\n",
    "        (Published)|\n",
    "        (AutoArchived)|\n",
    "        (Original reporting)|\n",
    "        (Audio reports)|\n",
    "        (Out of date stories)|\n",
    "        (News of the World)|\n",
    "        (Writing Contests)\n",
    "        ).*\"\"\", re.IGNORECASE | re.VERBOSE)\n",
    "        \n",
    "        filtered = {\n",
    "            key: value for key, value in categories.items() \n",
    "            if not date_regex.match(key) \n",
    "            and not year_regex.match(key) \n",
    "            and not wiki_regex.match(key) \n",
    "            and not wiki_other_noise.match(key)\n",
    "        }\n",
    "\n",
    "        filtered = {key: value for key, value in filtered.items() if len(value) >= min_articles}\n",
    "        return filtered\n",
    "\n",
    "    def __str__(self):\n",
    "        return '\"{}\":\\n\\tArticles: {}\\n\\tSubcategories: {}\\n'.format(self.name, self.articles, self.sub_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19737 articles loaded\n",
      "Categories before filtering: \t 5833\n",
      "Categories after filtering: \t 1612\n"
     ]
    }
   ],
   "source": [
    "from knowledgestore import ks\n",
    "from collections import defaultdict\n",
    "\n",
    "def articles_to_categories(articles):\n",
    "    categories = defaultdict(set)\n",
    "    for article in articles:\n",
    "        for category in article.categories:\n",
    "            categories[category].add(article)\n",
    "    return categories\n",
    "\n",
    "def create_article(link):\n",
    "    return Article(link[22:])\n",
    "\n",
    "FILE_CACHE = Path(\"articles.pickle\")\n",
    "if not FILE_CACHE.exists():\n",
    "    print(\"Generating articles for KnowledgeStore\")\n",
    "    with Pool(6) as pool:\n",
    "        articles = pool.map(create_article, ks.get_all_resource_uris())\n",
    "    with open(FILE_CACHE, \"wb\") as file:\n",
    "        pickle.dump(articles, file)\n",
    "else:\n",
    "    with open(FILE_CACHE, \"rb\") as file:\n",
    "        articles = pickle.load(file)\n",
    "    \n",
    "print(\"{} articles loaded\".format(len(articles)))\n",
    "\n",
    "categories = articles_to_categories(articles)\n",
    "print(\"Categories before filtering: \\t\", len(categories))\n",
    "categories = Category.filter_categories(categories, 5)\n",
    "print(\"Categories after filtering: \\t\", len(categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interests:\t\t ['Sony', 'Nairobi']\n",
      "\t\t\t \"Samsung to sell dual-standard DVD player\"\n",
      "\t\t\t \"American console sales continue to decline throughout April\"\n",
      "\t\t\t \"Sudanese parties sign peace pledge\"\n",
      "\t\t\t \"Mob attack on church in Kenya leaves 30 dead\"\n",
      "\t\t\t \"Robot goes to preschool\"\n",
      "\t\t\t \"Bombing in Kenya's capital city Nairobi\"\n",
      "Positive sample:\t \"Sony may cut PS3 prices\"\n",
      "Negative sample:\t \"A weak spot in HIV spotted\"\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class User:\n",
    "    def __init__(self, categories, num_interests):\n",
    "        self.__categories = categories\n",
    "        self.interests = random.sample(categories.keys(), num_interests)\n",
    "    \n",
    "    def get_positive_sample(self, num_articles_per_interest):\n",
    "        interests_articles = [\n",
    "            random.sample(self.__categories[interest], num_articles_per_interest + 1) \n",
    "            for interest in self.interests\n",
    "        ]\n",
    "        \n",
    "        input_data = []\n",
    "        true_labels = []\n",
    "        for articles in interests_articles:\n",
    "            true_labels.append(articles[0])\n",
    "            input_data.extend(articles[1:])\n",
    "            \n",
    "        random.shuffle(input_data)\n",
    "        return input_data, random.choice(true_labels)\n",
    "    \n",
    "    def get_negative_sample(self):\n",
    "        candidates = set()\n",
    "        while len(candidates) == 0:\n",
    "            negative_category = random.choice(tuple(categories.keys()))\n",
    "            if negative_category not in self.interests:\n",
    "                candidates = categories[negative_category]\n",
    "                for interest in self.interests:\n",
    "                    candidates = candidates.difference(categories[interest])\n",
    "        return random.choice(tuple(candidates))\n",
    "    \n",
    "user = User(categories, 2)\n",
    "interesting_articles, positive_sample = user.get_positive_sample(3)\n",
    "print(\"Interests:\\t\\t\", user.interests)\n",
    "for interesting_article in interesting_articles:\n",
    "    print(\"\\t\\t\\t\", interesting_article)\n",
    "print(\"Positive sample:\\t\", positive_sample)\n",
    "print(\"Negative sample:\\t\", user.get_negative_sample())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
